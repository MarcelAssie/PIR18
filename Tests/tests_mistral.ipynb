{
 "cells": [
  {
   "cell_type": "code",
   "id": "284d12b5ea8926f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:31:53.572839Z",
     "start_time": "2024-11-05T22:31:53.560206Z"
    }
   },
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM\n",
    "import os, re, time, pandas as pd"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1bc398dad530b82c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:31:54.108093Z",
     "start_time": "2024-11-05T22:31:53.584616Z"
    }
   },
   "source": [
    "def login_hf():\n",
    "    file_path = \"C:/Users/jeanm/Desktop/Ensg/Semestre3/ProjetRecherche/Ressources\"\n",
    "    api_keys = [api_keys for api_keys in os.listdir(file_path) if \"api_keys\" in api_keys]\n",
    "    with open(f\"{file_path}/{api_keys[0]}\", 'r', encoding='utf-8') as file:\n",
    "        file = file.read()\n",
    "        hugging_face_api_key= file.split(\"\\n\")[3]\n",
    "        return hugging_face_api_key\n",
    "    \n",
    "login(token=login_hf())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\jeanm\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T22:31:54.138835Z",
     "start_time": "2024-11-05T22:31:54.120131Z"
    }
   },
   "source": [
    "\n",
    "# Fonction pour lire le contenu d'un fichier texte\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Fonction pour découper un texte en morceaux de 512 tokens\n",
    "def chunk_text(text):\n",
    "    paragraphs = text.split(\"\\n\")  \n",
    "    results = []\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.strip():\n",
    "            results.append(paragraph)\n",
    "    return results\n",
    "\n",
    "# Fonction pour extraire les mots-clés\n",
    "def extract_keywords_from_chunks(chunks, test_model):\n",
    "    \n",
    "    # Charger le modèle et le tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "        model_file=test_model,\n",
    "        model_type=\"mistral\",\n",
    "        gpu_layers=0,\n",
    "        hf=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "    \n",
    "    generator = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task='text-generation',\n",
    "        max_new_tokens=50,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # Prompt pour extraire les mots-clés\n",
    "    keyword_prompt = \"\"\"\n",
    "    [INST]\n",
    "    \n",
    "    I have the following document:\n",
    "    - [DOCUMENT]\n",
    "    \n",
    "    Please extract only the keywords related to the Sustainable Development Goals (SDGs) that are explicitly mentioned in this document. \n",
    "    The keywords should consist of 2 to 3 words and should be meaningful within the context of this document. \n",
    "    Ensure that the keywords are derived solely from the text provided and do not include any external references or interpretations. \n",
    "    Return the keywords in a structured and readable format, without adding any extra explanations or phrases such as:\n",
    "    \"Here are the keywords present in the document.\"\n",
    "    [/INST]\n",
    "    \n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    count = 0\n",
    "    for chunk in chunks:\n",
    "        count += 1\n",
    "        print(f\"Traitement du paragraphe {count} sur {len(chunks)}\")\n",
    "        prompt = keyword_prompt.replace(\"[DOCUMENT]\", chunk)\n",
    "        llm = TextGeneration(generator, prompt=prompt)\n",
    "        kw_model = KeyLLM(llm)\n",
    "        extracted_keywords = kw_model.extract_keywords([chunk])\n",
    "\n",
    "        # Nettoyage préliminaire des mots-clés extraits\n",
    "        cleaned_keywords = [kw.replace('* ', '').strip() for kw in extracted_keywords[0][0].split('\\n') if kw]\n",
    "        keywords.append(cleaned_keywords)\n",
    "        print(f\"Fin de traitement du paragraghe {count}\")\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def clean_keywords(keywords, max_words=2):\n",
    "    # Étape 1: Nettoyer les mots-clés des caractères  \n",
    "    cleaned_keywords = []\n",
    "    for kw in keywords:\n",
    "        cleaned_kw = kw.lstrip('- ').strip()  \n",
    "        cleaned_keywords.append(cleaned_kw)    \n",
    "\n",
    "    # Étape 2: Supprimer les doublons tout en conservant l'ordre\n",
    "    cleaned_keywords = list(dict.fromkeys([kw.lower() for kw in cleaned_keywords]))\n",
    "\n",
    "    # Etape 3 : Retirer les numéros et points\n",
    "    filtered_keywords = [re.sub(r\"^\\d+\\.\\s*\", \"\", keyword) for keyword in cleaned_keywords]\n",
    "    \n",
    "    # Retirer les mots-clés contenant 'SGB', 'SGBS', 'Sustainable Development', 'indicator'\n",
    "    exclusion_terms = ['sdg','sdgs', 'sustainable development', 'sustainable development goals', 'indicator', 'sdg indicator']\n",
    "    filtered_keywords = [kw for kw in filtered_keywords if not any(excluded in kw.lower() for excluded in exclusion_terms)]\n",
    "\n",
    "    # Étape 4: Filtrer par nombre de mots\n",
    "    results = []\n",
    "    for kw in filtered_keywords:\n",
    "        if  len(kw.split()) == max_words and not re.search(r'\\d+', kw):\n",
    "            results.append(kw)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def start(txt_file, model):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Debut de traitement avec le modèle {model}...\\n\")\n",
    "    \n",
    "    # Charger le fichier texte\n",
    "    document_content = read_txt_file(txt_file[0])\n",
    "    \n",
    "    # Découper le texte en paragraphes\n",
    "    chunks = chunk_text(document_content)\n",
    "    \n",
    "    # Extraire les mots-clés de chaque morceau\n",
    "    keywords = extract_keywords_from_chunks(chunks, model)\n",
    "    \n",
    "     # Aplatir les mots-clés dans une unique liste\n",
    "    list_keywords = [keyword for sublist in keywords for keyword in sublist]\n",
    "    \n",
    "    # Nettoyer les mots-clés\n",
    "    cleaned_keywords = clean_keywords(list_keywords)\n",
    "    \n",
    "    # Créer un DataFrame avec les mots-clés détectés\n",
    "    keywords_df = pd.DataFrame(list_keywords, columns=[\"Keywords Detected\"])\n",
    "    \n",
    "    # Créer un DataFrame avec les mots-clés nettoyés\n",
    "    cleaned_keywords_df = pd.DataFrame(cleaned_keywords, columns=[\"Keywords Cleaned\"])\n",
    "\n",
    "    # Ajouter une colonne avec les mots-clés nettoyés, alignée sur les mots-clés détectés\n",
    "    keywords_df[\"Keywords Cleaned\"] = pd.Series(cleaned_keywords_df[\"Keywords Cleaned\"].values)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duree = end_time - start_time\n",
    "    print(f\"\\n...Fin de traitement avec le modèle {model}\\nDurée d'exécution : {duree:.4f} secondes.\\n\")\n",
    "    # display(keywords_df) \n",
    "   \n",
    "    return keywords_df\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "324ceef0701a8665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:31:54.171647Z",
     "start_time": "2024-11-05T22:31:54.165635Z"
    }
   },
   "source": [
    "# model1 = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
    "txt_file = [fichier for fichier in os.listdir() if fichier.lower().endswith('.txt')and \"Metadata\" in fichier]\n",
    "# start(txt_file, model1)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "aeed054142689ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:39:26.557535Z",
     "start_time": "2024-11-05T22:31:54.185047Z"
    }
   },
   "source": [
    "model2 = \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "start(txt_file, model2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debut de traitement avec le modèle mistral-7b-instruct-v0.1.Q5_K_M.gguf...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du paragraphe 1 sur 14\n",
      "Fin de traitement du paragraghe 1\n",
      "Traitement du paragraphe 2 sur 14\n",
      "Fin de traitement du paragraghe 2\n",
      "Traitement du paragraphe 3 sur 14\n",
      "Fin de traitement du paragraghe 3\n",
      "Traitement du paragraphe 4 sur 14\n",
      "Fin de traitement du paragraghe 4\n",
      "Traitement du paragraphe 5 sur 14\n",
      "Fin de traitement du paragraghe 5\n",
      "Traitement du paragraphe 6 sur 14\n",
      "Fin de traitement du paragraghe 6\n",
      "Traitement du paragraphe 7 sur 14\n",
      "Fin de traitement du paragraghe 7\n",
      "Traitement du paragraphe 8 sur 14\n",
      "Fin de traitement du paragraghe 8\n",
      "Traitement du paragraphe 9 sur 14\n",
      "Fin de traitement du paragraghe 9\n",
      "Traitement du paragraphe 10 sur 14\n",
      "Fin de traitement du paragraghe 10\n",
      "Traitement du paragraphe 11 sur 14\n",
      "Fin de traitement du paragraghe 11\n",
      "Traitement du paragraphe 12 sur 14\n",
      "Fin de traitement du paragraghe 12\n",
      "Traitement du paragraphe 13 sur 14\n",
      "Fin de traitement du paragraghe 13\n",
      "Traitement du paragraphe 14 sur 14\n",
      "Fin de traitement du paragraghe 14\n",
      "\n",
      "...Fin de traitement avec le modèle mistral-7b-instruct-v0.1.Q5_K_M.gguf\n",
      "Durée d'exécution : 452.3621 secondes.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                    Keywords Detected       Keywords Cleaned\n",
       "0   Here are the keywords related to the Sustainab...         climate change\n",
       "1                                                          poverty reduction\n",
       "2                                    - Climate change  grid/mains connection\n",
       "3                                         - Education         mobile devices\n",
       "4                                            - Health       desktop computer\n",
       "..                                                ...                    ...\n",
       "88                                       - Facilities                    NaN\n",
       "89                                             - Soap                    NaN\n",
       "90                                            - Water                    NaN\n",
       "91                                            - Girls                    NaN\n",
       "92                                             - Boys                    NaN\n",
       "\n",
       "[93 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keywords Detected</th>\n",
       "      <th>Keywords Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are the keywords related to the Sustainab...</td>\n",
       "      <td>climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>poverty reduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Climate change</td>\n",
       "      <td>grid/mains connection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Education</td>\n",
       "      <td>mobile devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Health</td>\n",
       "      <td>desktop computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>- Facilities</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>- Soap</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>- Water</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>- Girls</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>- Boys</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
