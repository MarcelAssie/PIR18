{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0932e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers\n",
      "Version: 3.1.1\n",
      "Summary: State-of-the-Art Text Embeddings\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Nils Reimers <info@nils-reimers.de>, Tom Aarsen <tom.aarsen@huggingface.co>\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\jeanm\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: huggingface-hub, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
      "Required-by: keybert\n"
     ]
    }
   ],
   "source": [
    "!pip show sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4998b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 2.0.2\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "        \n",
      "        Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "        All rights reserved.\n",
      "        \n",
      "        Copyright (c) 2011-2023, Open source contributors.\n",
      "        \n",
      "        Redistribution and use in source and binary forms, with or without\n",
      "        modification, are permitted provided that the following conditions are met:\n",
      "        \n",
      "        * Redistributions of source code must retain the above copyright notice, this\n",
      "          list of conditions and the following disclaimer.\n",
      "        \n",
      "        * Redistributions in binary form must reproduce the above copyright notice,\n",
      "          this list of conditions and the following disclaimer in the documentation\n",
      "          and/or other materials provided with the distribution.\n",
      "        \n",
      "        * Neither the name of the copyright holder nor the names of its\n",
      "          contributors may be used to endorse or promote products derived from\n",
      "          this software without specific prior written permission.\n",
      "        \n",
      "        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "        \n",
      "Location: c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: seaborn, statsmodels, swat, xarray\n"
     ]
    }
   ],
   "source": [
    "!pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffa5b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeanm\\AppData\\Roaming\\Python\\Python39\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1586e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f06b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1675a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea773964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filters import txt_to_string, filter_similar_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d70cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fuzzywuzzy\n",
      "Version: 0.18.0\n",
      "Summary: Fuzzy string matching in python\n",
      "Home-page: https://github.com/seatgeek/fuzzywuzzy\n",
      "Author: Adam Cohen\n",
      "Author-email: adam@seatgeek.com\n",
      "License: GPLv2\n",
      "Location: c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e933f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\program files\\arcgis\\pro\\bin\\python\\envs\\arcgispro-py3\\lib\\site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85847c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_extractions(dossier):\n",
    "    \"\"\"\n",
    "        Extrait les mots-clés de fichiers texte dans un dossier et enregistre les résultats dans des fichiers séparés.\n",
    "\n",
    "        Cette fonction parcourt un dossier donné, extrait le contenu de chaque fichier texte, puis utilise\n",
    "        le modèle KeyBERT pour extraire les mots-clés les plus pertinents. Les résultats sont ensuite sauvegardés\n",
    "        dans un fichier texte correspondant à chaque fichier analysé.\n",
    "\n",
    "        Paramètres:\n",
    "        -----------\n",
    "        dossier : str\n",
    "            Chemin vers le dossier contenant les fichiers texte (.txt).\n",
    "\n",
    "        Retour:\n",
    "        -------\n",
    "        None : La fonction ne retourne rien, mais crée des fichiers texte contenant les mots-clés et leurs scores.\n",
    "    \"\"\"\n",
    "    odd_number = re.search(r'\\d+', dossier).group()\n",
    "    print(\"------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Début de traitement de l'ODD {odd_number}...\\n\")\n",
    "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Liste des noms de fichiers texte dans le dossier\n",
    "    # J'utilise la condition supplementaire pour éviter de selectionner les fichiers traités\n",
    "    txt_files = [fichier for fichier in os.listdir(dossier)\n",
    "                 if fichier.endswith(\".txt\") and \"keywords\" not in fichier]\n",
    "\n",
    "    # Liste des données extraites\n",
    "    extracted_data = []\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        # Convertir le fichier texte en une chaîne de caractères\n",
    "        texte = txt_to_string(f\"{dossier}/{txt_file}\")\n",
    "        # Ajouter les données extraites dans la liste\n",
    "        extracted_data.append((txt_file, texte))\n",
    "        # print(extracted_data)\n",
    "        # break\n",
    "\n",
    "    # # Récupérer les numéros des cibles\n",
    "    # target_numbers = []\n",
    "    # for data in extracted_data:\n",
    "    #     if data[0][12:14] not in target_numbers:\n",
    "    #         target_numbers.append(data[0][12:14])\n",
    "    # # print(target_numbers)\n",
    "\n",
    "    keywords_ODD = []\n",
    "    for data in extracted_data:\n",
    "        # Récupérer le fichier texte extrait\n",
    "        doc = data[1]\n",
    "        # Configurer du modèle KeyBert pour la recherche de mots/phrases clé(e)s\n",
    "        kw_model = KeyBERT(model=sentence_model)\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            doc,\n",
    "            keyphrase_ngram_range=(1, 3),   # Taille de mots/phrases clé(e)s\n",
    "            stop_words='english',           # Eliminer les mots usuels en anglais\n",
    "            # use_maxsum=True,\n",
    "            nr_candidates=20,              # Nombre d'éléments à étudier\n",
    "            top_n=10                        # Nombre d'éléments choisis\n",
    "        )\n",
    "        # Suppression des entités similaires d'un point de vue orthographique\n",
    "        entities_filtered = filter_similar_entities(keywords, threshold=80)\n",
    "\n",
    "        # Filtrer les résultats par score\n",
    "        filtered_keywords = [(kw[0], kw[1], data[0][12:14]) for kw in keywords if kw[1] > 0.4]        # Ajouter les mots-clés extraits et leurs scores dans la liste des mots-clés pour l'ODD\n",
    "        keywords_ODD.extend(filtered_keywords)\n",
    "        # Chemin de sortie du fichier\n",
    "        metadata = data[0].replace(\".txt\",\"\")\n",
    "        output_path = f\"./{dossier}/{metadata}_keywords.txt\"\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Mots-clés extraits et scores :\\n\")\n",
    "            f.write(\"----------------------------------------------------\\n\")\n",
    "            for keyword in filtered_keywords:\n",
    "                f.write(f\"{keyword[0]} : {keyword[1]:.4f}\\n\")\n",
    "            f.write(\"----------------------------------------------------\\n\")\n",
    "\n",
    "    # Stockage en dataframe\n",
    "    df = pd.DataFrame(keywords_ODD, columns=[\"Mots-cles\", \"Scores\", \"Cibles\"])\n",
    "\n",
    "    # Enregistrer le DataFrame complet en CSV\n",
    "    output_path = f\"./{dossier}/ODD{odd_number}_keywords.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    # output_path = f\"./Keywords_odd/ODD{odd_number}.csv\"\n",
    "    output_path = f\"../Keywords_odd/ODD{odd_number}.csv\"\n",
    "    with open(output_path, mode=\"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Écrire l'en-tête\n",
    "        writer.writerow([\"Mots-cles\", \"Scores\", \"cibles\"])\n",
    "        # Écrire les mots-clés et leurs scores\n",
    "        for keyword in keywords_ODD:\n",
    "            writer.writerow([keyword[0], keyword[1],keyword[2]])\n",
    "\n",
    "\n",
    "    print(f\"...Fin de traitement de l'ODD {odd_number}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9badf116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Début de traitement de l'ODD 01...\n",
      "\n",
      "                     Mots-cles  Scores Cibles\n",
      "0   international poverty line  0.7255     01\n",
      "1      global absolute poverty  0.7141     01\n",
      "2  international poverty lines  0.7136     01\n",
      "3       global poverty poverty  0.7071     01\n",
      "4        international poverty  0.7054     01\n",
      "...Fin de traitement de l'ODD 01\n",
      "                       Mots-cles  Scores Cibles\n",
      "0     international poverty line  0.7255     01\n",
      "1        global absolute poverty  0.7141     01\n",
      "2    international poverty lines  0.7136     01\n",
      "3         global poverty poverty  0.7071     01\n",
      "4          international poverty  0.7054     01\n",
      "..                           ...     ...    ...\n",
      "155        monetary poor follows  0.6409     0b\n",
      "156                monetary poor  0.6316     0b\n",
      "157         monetary poor health  0.6047     0b\n",
      "158      transfers poor monetary  0.5886     0b\n",
      "159          cash transfers poor  0.5825     0b\n",
      "\n",
      "[160 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(entities_extractions(\"../MetaD/ODD01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9bca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
